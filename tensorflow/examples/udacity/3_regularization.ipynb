{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (400000, 28, 28) (400000,)\n",
      "Validation set (20000, 28, 28) (20000,)\n",
      "Test set (15000, 28, 28) (15000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (400000, 784) (400000, 10)\n",
      "Validation set (20000, 784) (20000, 10)\n",
      "Test set (15000, 784) (15000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1337)\n",
    "batch_size = 128\n",
    "hidden_nodes = 2048\n",
    "regularization = 0.0000001\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights_1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, hidden_nodes]))\n",
    "  biases_1 = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "\n",
    "\n",
    "  weights_2 = tf.Variable(\n",
    "    tf.truncated_normal([hidden_nodes, num_labels]))\n",
    "  biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  hidden = tf.nn.relu(tf.matmul(tf_train_dataset, weights_1) + biases_1)\n",
    "  logits = tf.matmul(hidden, weights_2) + biases_2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + tf.nn.l2_loss(logits) * regularization\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights_1) + biases_1), weights_2) + biases_2)\n",
    "  test_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights_1) + biases_1), weights_2) + biases_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 601.861450\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 32.4%\n",
      "Minibatch loss at step 1000: 28.121788\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 2000: 10.545779\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 82.8%\n",
      "Minibatch loss at step 3000: 15.521986\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 80.1%\n",
      "Minibatch loss at step 4000: 10.314809\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 83.8%\n",
      "Minibatch loss at step 5000: 5.474394\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 82.0%\n",
      "Minibatch loss at step 6000: 5.221628\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 82.4%\n",
      "Minibatch loss at step 7000: 2.061932\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 84.8%\n",
      "Minibatch loss at step 8000: 3.527903\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 9000: 1.748370\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 84.8%\n",
      "Minibatch loss at step 10000: 1.202536\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 11000: 0.936673\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 85.1%\n",
      "Minibatch loss at step 12000: 1.151003\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 84.8%\n",
      "Minibatch loss at step 13000: 1.176387\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at step 14000: 1.598953\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch loss at step 15000: 0.500937\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 16000: 1.773730\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 85.9%\n",
      "Minibatch loss at step 17000: 0.345751\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 18000: 0.809332\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at step 19000: 0.340473\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 20000: 0.354172\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 21000: 1.595668\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 22000: 1.313153\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 23000: 0.767327\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 24000: 1.827591\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 25000: 0.406242\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 26000: 0.461326\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 27000: 0.102060\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 28000: 0.900840\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 29000: 0.644866\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 30000: 0.718883\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 31000: 0.327357\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 32000: 0.605995\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 33000: 0.241149\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 34000: 0.684042\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 35000: 0.118611\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 36000: 0.387383\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 37000: 0.441837\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 38000: 0.147189\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 39000: 0.686739\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 40000: 0.246786\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 87.5%\n",
      "Test accuracy: 93.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 40001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 1000 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 570.019653\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 29.5%\n",
      "Minibatch loss at step 1000: 18.750111\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 70.7%\n",
      "Minibatch loss at step 2000: 0.522184\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 64.7%\n",
      "Minibatch loss at step 3000: 0.272634\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 55.2%\n",
      "Minibatch loss at step 4000: 0.376766\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 60.9%\n",
      "Minibatch loss at step 5000: 0.298442\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 59.1%\n",
      "Minibatch loss at step 6000: 0.268201\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 53.8%\n",
      "Minibatch loss at step 7000: 0.487896\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.5%\n",
      "Minibatch loss at step 8000: 0.177544\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 51.6%\n",
      "Minibatch loss at step 9000: 0.470336\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.3%\n",
      "Minibatch loss at step 10000: 0.081189\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 45.1%\n",
      "Minibatch loss at step 11000: 0.068242\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 40.0%\n",
      "Minibatch loss at step 12000: 0.033293\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 36.9%\n",
      "Minibatch loss at step 13000: 0.022770\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 34.7%\n",
      "Minibatch loss at step 14000: 0.026970\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 33.3%\n",
      "Minibatch loss at step 15000: 0.016641\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 32.1%\n",
      "Minibatch loss at step 16000: 0.012980\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 31.3%\n",
      "Minibatch loss at step 17000: 0.009238\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 30.5%\n",
      "Minibatch loss at step 18000: 0.008280\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 29.7%\n",
      "Minibatch loss at step 19000: 0.041281\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 29.2%\n",
      "Minibatch loss at step 20000: 0.006898\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 28.6%\n",
      "Minibatch loss at step 21000: 0.005493\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 28.2%\n",
      "Minibatch loss at step 22000: 0.004597\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 27.8%\n",
      "Minibatch loss at step 23000: 0.004648\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 27.5%\n",
      "Minibatch loss at step 24000: 0.007220\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 27.2%\n",
      "Minibatch loss at step 25000: 0.004230\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 26.9%\n",
      "Minibatch loss at step 26000: 0.003299\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 26.6%\n",
      "Minibatch loss at step 27000: 0.002977\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 26.4%\n",
      "Minibatch loss at step 28000: 0.003466\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 26.2%\n",
      "Minibatch loss at step 29000: 0.003131\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 26.0%\n",
      "Minibatch loss at step 30000: 0.002661\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 25.9%\n",
      "Minibatch loss at step 31000: 0.002338\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 25.7%\n",
      "Minibatch loss at step 32000: 0.002333\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 25.6%\n",
      "Minibatch loss at step 33000: 0.004552\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 25.4%\n",
      "Minibatch loss at step 34000: 0.002215\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 25.3%\n",
      "Minibatch loss at step 35000: 0.002013\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 25.1%\n",
      "Minibatch loss at step 36000: 0.001986\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 25.0%\n",
      "Minibatch loss at step 37000: 0.001930\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 24.9%\n",
      "Minibatch loss at step 38000: 0.008081\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 24.8%\n",
      "Minibatch loss at step 39000: 0.001767\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 24.7%\n",
      "Minibatch loss at step 40000: 0.001557\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 24.5%\n",
      "Test accuracy: 26.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 40001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (1000 - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 1000 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1337)\n",
    "batch_size = 256\n",
    "hidden_nodes = 4096\n",
    "regularization = 0.0000001\n",
    "dropout_keep = 0.8\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights_1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, hidden_nodes]))\n",
    "  biases_1 = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "\n",
    "\n",
    "  weights_2 = tf.Variable(\n",
    "    tf.truncated_normal([hidden_nodes, num_labels]))\n",
    "  biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  hidden = tf.nn.dropout(tf.nn.relu(tf.matmul(tf_train_dataset, weights_1) + biases_1), dropout_keep)\n",
    "  logits = tf.matmul(hidden, weights_2) + biases_2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + tf.nn.l2_loss(logits) * regularization\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights_1) + biases_1), weights_2) + biases_2)\n",
    "  test_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights_1) + biases_1), weights_2) + biases_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 684.708008\n",
      "Minibatch accuracy: 13.7%\n",
      "Validation accuracy: 38.5%\n",
      "Minibatch loss at step 1000: 38.186295\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 82.3%\n",
      "Minibatch loss at step 2000: 18.860067\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 84.0%\n",
      "Minibatch loss at step 3000: 12.878065\n",
      "Minibatch accuracy: 87.9%\n",
      "Validation accuracy: 83.9%\n",
      "Minibatch loss at step 4000: 6.013530\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 5000: 3.748379\n",
      "Minibatch accuracy: 89.5%\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 6000: 4.966045\n",
      "Minibatch accuracy: 87.9%\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 7000: 4.181520\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch loss at step 8000: 4.150757\n",
      "Minibatch accuracy: 87.9%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 9000: 5.611575\n",
      "Minibatch accuracy: 84.8%\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 10000: 4.733109\n",
      "Minibatch accuracy: 87.1%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 11000: 3.426058\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 12000: 1.752021\n",
      "Minibatch accuracy: 88.7%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 13000: 2.509945\n",
      "Minibatch accuracy: 89.5%\n",
      "Validation accuracy: 85.9%\n",
      "Minibatch loss at step 14000: 3.468931\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 15000: 1.922246\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 16000: 2.966925\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 17000: 1.374820\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 18000: 1.304274\n",
      "Minibatch accuracy: 88.7%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 19000: 2.171552\n",
      "Minibatch accuracy: 88.7%\n",
      "Validation accuracy: 87.9%\n",
      "Minibatch loss at step 20000: 2.415808\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.9%\n",
      "Minibatch loss at step 21000: 1.931755\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 22000: 0.787062\n",
      "Minibatch accuracy: 90.2%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 23000: 1.642373\n",
      "Minibatch accuracy: 91.0%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 24000: 3.356071\n",
      "Minibatch accuracy: 91.0%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 25000: 0.456814\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 26000: 0.470240\n",
      "Minibatch accuracy: 93.4%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 27000: 0.504060\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 28000: 1.940093\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 29000: 0.911142\n",
      "Minibatch accuracy: 91.8%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 30000: 0.371459\n",
      "Minibatch accuracy: 94.9%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 31000: 0.843251\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 32000: 1.081845\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 33000: 0.464320\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 34000: 0.663069\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 35000: 0.347576\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 36000: 0.888357\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 37000: 0.637297\n",
      "Minibatch accuracy: 94.1%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 38000: 0.351788\n",
      "Minibatch accuracy: 94.1%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 39000: 0.759785\n",
      "Minibatch accuracy: 93.4%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 40000: 1.014614\n",
      "Minibatch accuracy: 94.1%\n",
      "Validation accuracy: 87.5%\n",
      "Test accuracy: 92.4%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 40001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 1000 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7532.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "max_batch_size = 15000\n",
    "\n",
    "def lerp(a, b, t):\n",
    "    t = max(min(t, 1), 0)\n",
    "    return a + t * (b - a)\n",
    "\n",
    "display(lerp(batch_size, max_batch_size, 50000/100000.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1337)\n",
    "hidden_nodes = 1024\n",
    "hidden_nodes_2 = 1024\n",
    "batch_size = 2048\n",
    "max_batch_size = 4096\n",
    "\n",
    "def lerp(a, b, t):\n",
    "    t = max(min(t, 1), 0)\n",
    "    return a + t * (b - a)\n",
    "\n",
    "\n",
    "regularization = 0.000000000001\n",
    "dropout_keep = 1\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "\n",
    "  weights_1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, hidden_nodes]))\n",
    "  biases_1 = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "\n",
    "  weights_2 = tf.Variable(\n",
    "    tf.truncated_normal([hidden_nodes, hidden_nodes_2]))\n",
    "  biases_2 = tf.Variable(tf.zeros([hidden_nodes_2]))\n",
    "\n",
    "\n",
    "  weights_3 = tf.Variable(\n",
    "    tf.truncated_normal([hidden_nodes_2, num_labels]))\n",
    "  biases_3 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "\n",
    "\n",
    "  # Training computation.\n",
    "  hidden = tf.nn.dropout(tf.nn.relu(tf.matmul(tf_train_dataset, weights_1) + biases_1), dropout_keep)\n",
    "  hidden_2 = tf.nn.dropout(tf.nn.relu(tf.matmul(hidden, weights_2) + biases_2), dropout_keep)\n",
    "  logits = tf.matmul(hidden_2, weights_3) + biases_3\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + tf.nn.l2_loss(logits) * regularization\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf.nn.relu(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights_1) + biases_1),weights_2) + biases_2), weights_3) + biases_3)\n",
    "  test_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf.nn.relu(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights_1) + biases_1),weights_2) + biases_2), weights_3) + biases_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 6634.019043\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 23.0%\n",
      "Minibatch loss at step 4000: 1.526056\n",
      "Minibatch accuracy: 65.0%\n",
      "Validation accuracy: 63.3%\n",
      "Minibatch loss at step 8000: 1.050908\n",
      "Minibatch accuracy: 69.0%\n",
      "Validation accuracy: 69.7%\n",
      "Minibatch loss at step 12000: 1.186167\n",
      "Minibatch accuracy: 71.8%\n",
      "Validation accuracy: 72.4%\n",
      "Minibatch loss at step 16000: 0.821448\n",
      "Minibatch accuracy: 73.5%\n",
      "Validation accuracy: 75.1%\n",
      "Minibatch loss at step 20000: 0.716975\n",
      "Minibatch accuracy: 76.0%\n",
      "Validation accuracy: 75.9%\n",
      "Minibatch loss at step 24000: 0.704490\n",
      "Minibatch accuracy: 77.5%\n",
      "Validation accuracy: 76.5%\n",
      "Minibatch loss at step 28000: 0.745552\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 32000: 0.701176\n",
      "Minibatch accuracy: 77.8%\n",
      "Validation accuracy: 78.0%\n",
      "Minibatch loss at step 36000: 0.639617\n",
      "Minibatch accuracy: 78.7%\n",
      "Validation accuracy: 78.7%\n",
      "Minibatch loss at step 40000: 0.660003\n",
      "Minibatch accuracy: 79.0%\n",
      "Validation accuracy: 78.7%\n",
      "Minibatch loss at step 44000: 0.624999\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 79.2%\n",
      "Minibatch loss at step 48000: 0.649332\n",
      "Minibatch accuracy: 78.6%\n",
      "Validation accuracy: 79.3%\n",
      "Minibatch loss at step 52000: 0.637609\n",
      "Minibatch accuracy: 80.2%\n",
      "Validation accuracy: 80.0%\n",
      "Minibatch loss at step 56000: 0.580060\n",
      "Minibatch accuracy: 81.5%\n",
      "Validation accuracy: 80.3%\n",
      "Minibatch loss at step 60000: 0.626784\n",
      "Minibatch accuracy: 80.0%\n",
      "Validation accuracy: 80.2%\n",
      "Minibatch loss at step 64000: 0.557842\n",
      "Minibatch accuracy: 81.9%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 68000: 0.539639\n",
      "Minibatch accuracy: 82.1%\n",
      "Validation accuracy: 81.1%\n",
      "Minibatch loss at step 72000: 0.699145\n",
      "Minibatch accuracy: 79.0%\n",
      "Validation accuracy: 79.7%\n",
      "Minibatch loss at step 76000: 0.537253\n",
      "Minibatch accuracy: 82.9%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 80000: 0.552280\n",
      "Minibatch accuracy: 82.4%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 84000: 0.646624\n",
      "Minibatch accuracy: 80.4%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 88000: 0.559299\n",
      "Minibatch accuracy: 82.5%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 92000: 0.480300\n",
      "Minibatch accuracy: 84.9%\n",
      "Validation accuracy: 82.1%\n",
      "Minibatch loss at step 96000: 0.562967\n",
      "Minibatch accuracy: 81.8%\n",
      "Validation accuracy: 82.0%\n",
      "Minibatch loss at step 100000: 0.497447\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 82.3%\n",
      "Minibatch loss at step 104000: 0.493295\n",
      "Minibatch accuracy: 84.1%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 108000: 0.522590\n",
      "Minibatch accuracy: 83.7%\n",
      "Validation accuracy: 82.4%\n",
      "Minibatch loss at step 112000: 0.516522\n",
      "Minibatch accuracy: 83.2%\n",
      "Validation accuracy: 82.3%\n",
      "Minibatch loss at step 116000: 0.505169\n",
      "Minibatch accuracy: 83.2%\n",
      "Validation accuracy: 82.4%\n",
      "Minibatch loss at step 120000: 0.513489\n",
      "Minibatch accuracy: 83.1%\n",
      "Validation accuracy: 82.4%\n",
      "Minibatch loss at step 124000: 0.500133\n",
      "Minibatch accuracy: 83.7%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 128000: 0.532004\n",
      "Minibatch accuracy: 83.0%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 132000: 0.498083\n",
      "Minibatch accuracy: 84.2%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 136000: 0.462903\n",
      "Minibatch accuracy: 85.5%\n",
      "Validation accuracy: 82.8%\n",
      "Minibatch loss at step 140000: 0.507051\n",
      "Minibatch accuracy: 83.2%\n",
      "Validation accuracy: 82.8%\n",
      "Minibatch loss at step 144000: 0.486399\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 148000: 0.465190\n",
      "Minibatch accuracy: 85.3%\n",
      "Validation accuracy: 82.9%\n",
      "Test accuracy: 89.7%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEACAYAAAC57G0KAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRERERERERERERETC9X+lWMf/51kvhgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feb53296bd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_steps = 150001\n",
    "\n",
    "validation_predictions = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    current_batch_size = int(lerp(batch_size, max_batch_size, step/float(num_steps)))\n",
    "    offset = (step * current_batch_size) % (train_labels.shape[0] - current_batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    learning_rate_value = lerp(0.045, 0.001, step/float(num_steps))\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, learning_rate: learning_rate_value}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 100 == 0):\n",
    "      validation_predictions.append(accuracy(predictions, batch_labels))\n",
    "    if (step % 4000 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "  pyplot.plot(np.arange(len(validation_predictions)), validation_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7febaea09a90>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAEACAYAAACuzv3DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADO5JREFUeJzt3V+IHPUBwPHvVu2DRghWSWKNnsSWWhDsS2ixxX2QGF9a\nfbEUejnbUPpQtOB/X3JHa8DqWYQWzqZViVIsgijJS0msLtiXFuOdRiVagynaaiwqtH1xpU4fZmIm\n93dndv78Zub7gSWzc3e7PyaT333vt7MXkCRJkiRJkiRJkiRJkiRJKtRm4DngVeAV4KZk/znAQeAN\n4ACwvpbRSZLWtBG4PNleB7wOXArcC9ye7L8DuKf6oUmS8ngauAo4AmxI9m1M7kuSAjcB/B04G/go\ntb+36L4kKUDrgEPAtcn9xRP3h9UOR5J0wukjfM4ZwJPAY8RLKwDHiZdU3gM2Ae8v/qItW7ZER48e\nLWiYktQZR4FLsnzB59b4eA94CHgNeCC1fx8wlWxPcXKCPzmSo0eJoshbQbfp6enax9Cmm8fT4xnC\n7eOPI3btijjvvIi9eyM+/TQC2JJlEoe1i/wK4PvAy8B8su8u4qtUngB2AseA67M+sSR12cIC3HAD\nXHBBvH3++fkfa62J/M+sXO1X5X9aSeqm4RB274a5OZidhclJ6PXGe8xR1sgVgH6/X/cQWsXjWSyP\n52iKrPC0Mb8PrCqKoqjEh5ekZshS4b34A5nmZotckkpUVoWnrXXViiQph+EQpqdh2za4+WbYv7+c\nSRwsckkqXBUVnmaRS1JBqqzwNItckgpQdYWnWeSSNIa6KjzNIpeknOqs8DSLXJIyCqHC0yxyScog\nlApPs8glaQShVXiaRS5JawixwtMscklaQcgVnmaRS9IyQq/wNItcklKaUuFpFrkkJZpU4WkWuaTO\na2KFp1nkkjqtqRWeZpFL6qSmV3iaRS6pc9pQ4WkWuaTOaFOFp1nkkjqhbRWeZpFLarW2VniaRS6p\ntdpc4WkWuaTW6UKFp1nkklqlKxWeZpFLaoWuVXiaRS6p8bpY4WkWuaTG6nKFp1nkkhqp6xWeZpFL\nahQrfCmLXFJjWOHLs8glBc8KX51FLiloVvjaLHJJQbLCR2eRSwqOFZ6NRS4pGFZ4Pha5pCBY4flZ\n5JJqZYWPzyKXVBsrvBgWuaTKWeHFssglVcoKL55FLqkSVnh5LHJJpbPCy2WRSyqNFV4Ni1xSKazw\n6oxS5A8Dx4HDqX0zwDvAfHLbXvjIJDWSFV69UYr8EeBXwKOpfRHwy+QmSYAVXpdRivx54KNl9vcK\nHoukhrLC6zXOi503Ai8BDwHrixmOpKZZWICtW+HQoXh7xw7omXmVyvti5xzws2T758D9wM7FnzQz\nM/PZdr/fp9/v53w6SaEZDmH3bpibg9lZmJx0As9jMBgwGAzGeoxRD/sEsB+4LMPHoiiKcg9MUrjS\na+F79riMUqRe/N0w07fEvEsrm1Lb13HqFS2SWsq18DCNsrTyOHAlcC7wNjAN9IHLia9eeQv4cUnj\nkxQIr0gJV5krWi6tSC3gWni18iyt+M5OSSuywpvB37UiaQnXwpvFIpd0Ciu8eSxySYAV3mQWuSQr\nvOEscqnDrPB2sMiljrLC28MilzrGCm8fi1zqECu8nSxyqQOs8HazyKWWs8LbzyKXWsoK7w6LXGoh\nK7xbLHKpRazwbrLIpZawwrvLIpcazgqXRS41mBUusMilRrLClWaRSw1jhWsxi1xqCCtcK7HIpQaw\nwrUai1wKmBWuUVjkUqCscI3KIpcCY4UrK4tcCogVrjwscikAVrjGYZFLNbPCNS6LXKqJFa6iWORS\nDaxwFckilypkhasMFrlUEStcZbHIpZJZ4SqbRS6VyApXFSxyqQRWuKpkkUsFs8JVNYtcKogVrrpY\n5FIBrHDVySKXxmCFKwQWuZSTFa5QWORSRla4QmORSxlY4QqRRS6NwApXyCxyaQ1WuEJnkUsrsMLV\nFBa5tAwrXE1ikUspVriayCKXEla4mmqUIn8YOA4cTu07BzgIvAEcANYXPzSpGla4mm6UifwRYPui\nfXcST+RfBv6U3JcaZ2EBtm6FQ4fi7R07oNere1RSNqNM5M8DHy3a921gb7K9F7i2yEFJZbPC1SZ5\n18g3EC+3kPy5oZjhSOVzLVxtU8SLnVFyW2JmZuaz7X6/T7/fL+DppHyGQ9i9G+bm4L77XEZRGAaD\nAYPBYKzHGPU0ngD2A5cl948AfeA9YBPwHPCVRV8TRdGy87tUuXSF79ljhStcvbguMiVG3uvI9wFT\nyfYU8HTOx5FK5Vq4umCUpZXHgSuBc4G3gV3APcATwE7gGHB9SeOTcnMtXF1R5gqhSyuqRXotfHYW\nJiddC1dz5Fla8Z2dahUrXF3k71pRK7gWri6zyNV4Vri6ziJXY1nhUswiVyNZ4dJJFrkaxQqXlrLI\n1RhWuLQ8i1zBs8Kl1VnkCtr8fFzhmzdb4dJKLHIF6USFX3013HKLFS6txiJXcKxwKRuLXMGwwqV8\nLHIFwQqX8rPIVSsrXBqfRa7aWOFSMSxyVc4Kl4plkatSVrhUPItclbDCpfJY5CqdFS6VyyJXaaxw\nqRoWuUphhUvVschVKCtcqp5FrsJY4VI9LHKNzQqX6mWRayxWuFQ/i1y5WOFSOCxyZWaFS2GxyDUy\nK1wKk0WukVjhUrgscq3KCpfCZ5FrRVa41AwWuZawwqVmsch1Citcah6LXIAVLjWZRS4rXGo4i7zD\nrHCpHSzyjrLCpfawyDtmOIRdu6xwqU0s8g558cW4wi+80AqX2sQi74ATFb59O9x6qxUutY1F3nJW\nuNR+FnlLWeFSd1jkLWSFS91ikbeIFS51k0XeEla41F0WecNZ4ZIs8gazwiXB+BP5MeDfwP+AT4Ct\n4w5IaxsO4e674cEHYXYWJieh16t7VJLqMu5EHgF94MPxh6JRWOGSFitijdwWrIBr4ZJWUkSRP0O8\ntPIb4Ldjj0hLzM/D1JQVLml5407kVwDvAucBB4EjwPMnPjgzM/PZJ/b7ffr9/phP1y2uhUvtNxgM\nGAwGYz1GkdPCNPBf4P7kfhRFUYEP3y3ptfA9e6xwqSt6ca1lmpvHWSM/Ezg72T4L2AYcHuPxhGvh\nkrIbZ2llA/BU6nF+DxwYe0Qd5hUpkvIoc8XVpZURuRYu6YQ8Syu+s7NmVrikcfm7VmriWrikoljk\nNbDCJRXJIq+QFS6pDBZ5RaxwSWWxyEtmhUsqm0VeIitcUhUs8hJY4ZKqZJEXzAqXVDWLvCBWuKS6\nWOQFsMIl1ckiH4MVLikEFnlOVrikUFjkGVnhkkJjkWdghUsKkUU+AitcUsgs8jVY4ZJCZ5GvIF3h\nt91mhUsKl0W+jBMVftFFVrik8FnkKYsrfN8+J3FJ4bPIE1a4pKbqfJFb4ZKartNFboVLaoNOFrkV\nLqlNOlfkVriktulMkVvhktqqE0VuhUtqs1YXuRUuqQtaW+RWuKSuaF2RW+GSuqZVRW6FS+qiVhS5\nFS6pyxpf5Fa4pK5rbJFb4ZIUa2SRW+GSdFKjitwKl6SlGlPkVrgkLS/4IrfCJWl1QRe5FS5Jawu2\nyJ991gqXpFH0SnzsKIqi3F88HMIHH8CmTQWOSJIC1+v1IOPcHOxELkldlGciD3ZpRZI0GidySWo4\nJ3JJajgncklquHEm8u3AEeBvwB3FDEeSlFXeifw04NfEk/lXge8BlxY1KC01GAzqHkKreDyL5fGs\nV96JfCvwJnAM+AT4A/CdgsakZfgPpVgez2J5POuVdyL/IvB26v47yT5JUsXyTuS+00eSApH3nZ1f\nB2aI18gB7gI+BX6R+pw3gS25RyZJ3XQUuKSKJzo9ebIJ4PPAAr7YKUmNcw3wOnF531XzWCRJkiSl\n+WahYh0DXgbmgb/WO5TGeRg4DhxO7TsHOAi8ARwA1tcwrqZa7njOEF+5Np/cti/9Mq1gM/Ac8Crw\nCnBTsr/2c/Q04uWWCeAMXD8vwlvEf7HK7lvA1zh14rkXuD3ZvgO4p+pBNdhyx3MauLme4TTeRuDy\nZHsd8XL1pQRwjn4D+GPq/p3JTfm9BXyh7kE02ASnTjxHgA3J9sbkvkY3wdKJ/JZ6htI6TwNXkfEc\nLeOXZvlmoeJFwDPAC8CPah5LG2wgXh4g+XPDKp+r0dwIvAQ8hEtVeU0Q/7TzFzKeo2VM5L5ZqHhX\nEP8FXwP8hPjHWxUjwnN2XHPAxcRLBO8C99c7nEZaBzwJ/BT4z6KPrXmOljGR/4N4Af+EzcRVrvze\nTf78F/AU8e+6UX7HiX9cBdgEvF/jWNrgfU5ONr/D8zOrM4gn8ceIl1Yg4zlaxkT+AvAlTr5Z6LvA\nvhKepyvOBM5Ots8CtnHq+qSy2wdMJdtTnPzHo3zS/0X6dXh+ZtEjXo56DXggtT+Ic9Q3CxXnYuIr\nfxaIL0/yeGbzOPBPYEj82s0PiK8AegYvP8xj8fH8IfAo8eWxLxFPOL7mMLpvEv96kwVOvXzTc1SS\nJEmSJEmSJEmSJEmSJEmSJElqq/8DGf9tjjWoSMcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7febb10f3390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a=[1,5,10,20]\n",
    "%matplotlib inline\n",
    "from IPython.display import display, Image\n",
    "pyplot.plot(a, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
